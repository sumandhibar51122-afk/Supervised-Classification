{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "**Information Gain (IG)** is a metric used in **Decision Trees** to decide which feature should be chosen as a splitting point at each node.\n",
        "\n",
        "---\n",
        "\n",
        "## **What is Information Gain?**\n",
        "\n",
        "Information Gain measures **how much uncertainty (entropy) is reduced** in the target variable after splitting the data based on a particular feature.\n",
        "\n",
        "* **Entropy** quantifies impurity or disorder in the dataset.\n",
        "* A good split **reduces entropy**, making the resulting subsets more pure.\n",
        "\n",
        "### **Formula**\n",
        "\n",
        "[\n",
        "IG(S, A) = Entropy(S) - \\sum_{v\\in Values(A)} \\frac{|S_v|}{|S|}Entropy(S_v)\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (S) = original dataset\n",
        "* (A) = feature\n",
        "* (S_v) = subset of data where feature (A) has value (v)\n",
        "\n",
        "---\n",
        "\n",
        "## **How is Information Gain used in Decision Trees?**\n",
        "\n",
        "1. **Compute entropy** of the current dataset.\n",
        "2. **For each candidate feature**, compute the expected entropy after splitting on that feature.\n",
        "3. **Calculate Information Gain** for each feature.\n",
        "4. **Select the feature with the highest Information Gain** as the decision node.\n",
        "5. **Repeat recursively** for each child node until stopping criteria are met (e.g., max depth, pure node).\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Information Gain?**\n",
        "\n",
        "* Helps choose splits that **best separate classes**.\n",
        "* Encourages creation of **pure child nodes**.\n",
        "* Leads to a more **accurate and efficient** decision tree.\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "X7og-zDYhona"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "**Gini Impurity** and **Entropy** are both measures of impurity used in decision tree algorithms (e.g., CART, ID3, C4.5) to determine the best split. They serve the same purpose but differ in formulation, behavior, and computational complexity.\n",
        "\n",
        "---\n",
        "\n",
        "# **1. Definitions**\n",
        "\n",
        "## **Gini Impurity**\n",
        "\n",
        "Measures how often a randomly chosen element would be incorrectly labeled if it were labeled according to the distribution of labels in the dataset.\n",
        "\n",
        "[\n",
        "Gini = 1 - \\sum_{i=1}^{k} p_i^2\n",
        "]\n",
        "\n",
        "* (p_i): probability of class (i)\n",
        "* Lower Gini ⇒ more pure.\n",
        "\n",
        "---\n",
        "\n",
        "## **Entropy**\n",
        "\n",
        "Measures the amount of disorder or uncertainty in the dataset.\n",
        "\n",
        "[\n",
        "Entropy = -\\sum_{i=1}^{k} p_i \\log_2 p_i\n",
        "]\n",
        "\n",
        "* Based on information theory.\n",
        "* Higher entropy ⇒ more disorder.\n",
        "\n",
        "---\n",
        "\n",
        "# **2. Key Differences**\n",
        "\n",
        "| Aspect                      | Gini Impurity                            | Entropy                                            |\n",
        "| --------------------------- | ---------------------------------------- | -------------------------------------------------- |\n",
        "| **Formula Type**            | Uses squared probabilities               | Uses log probabilities                             |\n",
        "| **Computation Cost**        | Faster (no logarithms)                   | Slower (uses logarithms)                           |\n",
        "| **Decision Tree Algorithm** | Used in CART                             | Used in ID3, C4.5                                  |\n",
        "| **Range**                   | 0 to (1 – 1/k)                           | 0 to log₂(k)                                       |\n",
        "| **Bias**                    | Tends to isolate the most frequent class | More sensitive to distribution changes             |\n",
        "| **Behavior**                | Creates “purer” nodes slightly faster    | More mathematically precise measure of uncertainty |\n",
        "\n",
        "---\n",
        "\n",
        "# **3. Intuition**\n",
        "\n",
        "### **Gini Impurity**\n",
        "\n",
        "* Measures *probability of misclassification*.\n",
        "* Smooth, convex function.\n",
        "* Prefers splits that isolate dominant classes.\n",
        "\n",
        "### **Entropy**\n",
        "\n",
        "* Measures *information content or surprise*.\n",
        "* Penalizes impurity more strongly when classes are evenly mixed.\n",
        "\n",
        "---\n",
        "\n",
        "# **4. Practical Impact**\n",
        "\n",
        "* Often **both criteria yield very similar trees**.\n",
        "* Gini is preferred when **speed** is important.\n",
        "* Entropy is preferred when **theoretical grounding in information theory** matters.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jBYkPThjiAa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "**Pre-pruning** (also called **early stopping**) is a technique used in decision tree learning to **stop the tree from growing too deep** before it begins to overfit the training data.\n",
        "\n",
        "---\n",
        "\n",
        "#  **Definition**\n",
        "\n",
        "**Pre-pruning** prevents additional splitting of a node **if the split does not provide enough improvement** according to some criteria.\n",
        "\n",
        "Rather than fully growing the tree and then pruning it, pre-pruning stops the growth **during** training.\n",
        "\n",
        "---\n",
        "\n",
        "#  **Why Pre-Pruning?**\n",
        "\n",
        "* Prevents **overfitting**\n",
        "* Reduces **model complexity**\n",
        "* Improves **generalization**\n",
        "* Reduces training time\n",
        "\n",
        "---\n",
        "\n",
        "#  **Common Pre-Pruning Techniques**\n",
        "\n",
        "### **1. Minimum Information Gain / Minimum Impurity Decrease**\n",
        "\n",
        "Stop splitting a node if:\n",
        "\n",
        "* Information gain < threshold\n",
        "* Gini or entropy reduction < threshold\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Maximum Depth Limit**\n",
        "\n",
        "Restrict how deep the tree can grow.\n",
        "\n",
        "Example: `max_depth = 10`\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Minimum Samples per Node**\n",
        "\n",
        "Stop splitting if a node contains fewer than a specified number of samples.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* `min_samples_split = 20`\n",
        "* `min_samples_leaf = 5`\n",
        "\n",
        "---\n",
        "\n",
        "**4. Early stopping based on statistical tests**\n",
        "\n",
        "Use tests like **Chi-square** to check if a split is statistically significant.\n",
        "If not, the node is not split.\n",
        "\n",
        "---\n",
        "\n",
        "**How Pre-Pruning Works (Process)**\n",
        "\n",
        "1. At each node, evaluate all possible splits.\n",
        "2. If the best split **does not meet pruning criteria**, stop splitting.\n",
        "3. Convert the node into a leaf.\n",
        "4. Continue for other branches.\n",
        "\n",
        "---\n",
        "\n",
        "If splitting a node reduces entropy by only **0.0005**, and the threshold for minimum information gain is **0.001**,\n",
        "➡️ The node will **not** be split.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Pre-pruning significantly reduces the chance of overfitting by **controlling unnecessary branching early**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GXE8Tm6-kUXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "#Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Optional: Print a\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuGcRu7XkzWl",
        "outputId": "2e2c2cff-b516-44fa-ec8d-7d9afd57e041"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification** and **regression**, though it is most commonly used for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "#  **Definition**\n",
        "\n",
        "A **Support Vector Machine** tries to find the **best separating boundary (hyperplane)** between classes so that the margin (distance between the boundary and the closest data points) is **maximized**.\n",
        "\n",
        "Those closest data points are called **support vectors**, and they determine the position and orientation of the decision boundary.\n",
        "\n",
        "---\n",
        "\n",
        "# **Key Idea**\n",
        "\n",
        "SVM finds a hyperplane that:\n",
        "\n",
        "* Best separates classes\n",
        "* Maximizes the **margin**\n",
        "* Reduces classification error\n",
        "\n",
        "This makes SVM a **maximum-margin classifier**.\n",
        "\n",
        "---\n",
        "\n",
        "# **How SVM Works**\n",
        "\n",
        "1. For linearly separable data, it finds a straight line (or plane) that separates classes with the **maximum margin**.\n",
        "2. For non-linear data, SVM uses **kernel functions** to transform data into a higher-dimensional space where separation becomes possible.\n",
        "\n",
        "---\n",
        "\n",
        "#  **Important Components**\n",
        "\n",
        "### **1. Hyperplane**\n",
        "\n",
        "A decision boundary that separates classes.\n",
        "\n",
        "* In 2D → a line\n",
        "* In 3D → a plane\n",
        "* In higher dimensions → a hyperplane\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Support Vectors**\n",
        "\n",
        "The data points closest to the hyperplane.\n",
        "\n",
        "* They “support” the decision boundary.\n",
        "* Removing them would change the hyperplane.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Margin**\n",
        "\n",
        "The distance between the support vectors and the hyperplane.\n",
        "\n",
        "* SVM tries to **maximize the margin**, improving generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Kernel Trick**\n",
        "\n",
        "Allows SVM to handle non-linear classification by projecting data into higher dimensions.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "* **Linear Kernel**\n",
        "* **Polynomial Kernel**\n",
        "* **RBF (Gaussian) Kernel**\n",
        "* **Sigmoid Kernel**\n",
        "\n",
        "---\n",
        "\n",
        "#  **Why Use SVM?**\n",
        "\n",
        "* Works well for high-dimensional data\n",
        "* Effective when classes are separable\n",
        "* Robust against overfitting, especially in high-dimensional spaces\n",
        "* Can model complex boundaries using kernels\n",
        "\n",
        "---\n",
        "\n",
        "#  **Summary**\n",
        "\n",
        "A **Support Vector Machine** is a powerful classifier that:\n",
        "\n",
        "* Identifies the optimal separating hyperplane\n",
        "* Maximizes the margin between classes\n",
        "* Uses kernel functions for non-linear data\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9gxdSeylDso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "The **Kernel Trick** is a technique used in **Support Vector Machines (SVMs)** to handle data that is **not linearly separable** in its original feature space.\n",
        "\n",
        "---\n",
        "\n",
        "#  **Definition**\n",
        "\n",
        "The **Kernel Trick** allows SVM to compute distances or similarities in a **high-dimensional feature space** **without explicitly transforming the data** into that space.\n",
        "\n",
        "Instead of mapping data to higher dimensions, SVM uses a **kernel function** that computes the **inner product** of two points *as if* they were transformed.\n",
        "\n",
        "This makes SVM powerful and computationally efficient.\n",
        "\n",
        "---\n",
        "\n",
        "#  **Why Do We Need the Kernel Trick?**\n",
        "\n",
        "Some datasets cannot be separated with a straight line (linear boundary).\n",
        "For example:\n",
        "\n",
        "* XOR pattern\n",
        "* Concentric circles\n",
        "* Spiral patterns\n",
        "\n",
        "Mapping data to a higher dimension can make it linearly separable — but doing this explicitly is computationally expensive.\n",
        "\n",
        "The kernel trick bypasses this by **computing the high-dimensional relationships directly**.\n",
        "\n",
        "---\n",
        "\n",
        "#  **How It Works (Intuition)**\n",
        "\n",
        "Instead of doing:\n",
        "\n",
        "[\n",
        "\\phi(x) \\rightarrow \\text{high dimensional transform}\n",
        "]\n",
        "\n",
        "SVM uses a kernel to directly compute:\n",
        "\n",
        "[\n",
        "K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle\n",
        "]\n",
        "\n",
        "This avoids the costly computation of (\\phi(x)), especially when the feature space is huge or infinite.\n",
        "\n",
        "---\n",
        "\n",
        "#  **Common Kernel Functions**\n",
        "\n",
        "### **1. Linear Kernel**\n",
        "\n",
        "[\n",
        "K(x, x') = x^T x'\n",
        "]\n",
        "Used when data is linearly separable.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Polynomial Kernel**\n",
        "\n",
        "[\n",
        "K(x, x') = (x^T x' + c)^d\n",
        "]\n",
        "Captures polynomial relationships.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. RBF (Radial Basis Function) / Gaussian Kernel**\n",
        "\n",
        "[\n",
        "K(x, x') = e^{-\\gamma ||x - x'||^2}\n",
        "]\n",
        "Most common; handles highly non-linear data.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Sigmoid Kernel**\n",
        "\n",
        "[\n",
        "K(x, x') = \\tanh(\\alpha x^T x' + c)\n",
        "]\n",
        "Similar to neural network activation.\n",
        "\n",
        "---\n",
        "\n",
        "#  **Benefits of the Kernel Trick**\n",
        "\n",
        "* Allows SVM to solve **non-linear classification problems**\n",
        "* Avoids the computational cost of high-dimensional feature mapping\n",
        "* Enables SVM to work efficiently with complex boundaries\n",
        "* Often gives high accuracy on intricate datasets\n",
        "\n",
        "---\n",
        "\n",
        "#  **In One Sentence**\n",
        "\n",
        "The **Kernel Trick** lets SVM classify non-linear data by computing high-dimensional relationships without performing the actual transformation into that high-dimensional space.\n",
        "\n"
      ],
      "metadata": {
        "id": "jN3PK4e-lUyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#kernels on the Wine dataset, then compare their accuracies.\n",
        "#Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale the data (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test_scaled))\n",
        "\n",
        "# SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Accuracy with Linear Kernel: {linear_acc:.4f}\")\n",
        "print(f\"Accuracy with RBF Kernel: {rbf_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxKa47AdlsnQ",
        "outputId": "14e1c14e-ebc2-4aff-c977-7b5ef2706845"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9722\n",
            "Accuracy with RBF Kernel: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "The **Naïve Bayes classifier** is a **probabilistic machine learning algorithm** based on **Bayes' Theorem**, used mainly for **classification** tasks such as spam detection, sentiment analysis, and document categorization.\n",
        "\n",
        "---\n",
        "\n",
        "#  **What is the Naïve Bayes Classifier?**\n",
        "\n",
        "It predicts the class of a data point by calculating the **posterior probability** for each class using Bayes' Theorem:\n",
        "\n",
        "[\n",
        "P(C|X) = \\frac{P(X|C) , P(C)}{P(X)}\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (C) = class\n",
        "* (X) = features of the input data\n",
        "\n",
        "Naïve Bayes chooses the class with the **highest posterior probability**.\n",
        "\n",
        "It is simple, efficient, and works well on high-dimensional data (e.g., text).\n",
        "\n",
        "---\n",
        "\n",
        "#  **Why is it called \"Naïve\"?**\n",
        "\n",
        "It is called **Naïve** because it makes a **strong assumption**:\n",
        "\n",
        "###  **All features are conditionally independent given the class.**\n",
        "\n",
        "That means:\n",
        "\n",
        "* It assumes each feature contributes to the probability **independently**, even if that is not true in real data.\n",
        "\n",
        "Example:\n",
        "If predicting whether an email is spam, Naïve Bayes assumes that the presence of the words \"free\", \"offer\", and \"money\" are all independent events—even though they often occur together.\n",
        "\n",
        "This assumption is rarely true → hence the name **\"Naïve.\"**\n",
        "\n",
        "---\n",
        "\n",
        "#  **Despite the naive assumption, it works surprisingly well**\n",
        "\n",
        "* Efficient for large datasets\n",
        "* Performs well on text data\n",
        "* Requires small amount of training data\n",
        "* Robust even when independence assumption is violated\n",
        "\n",
        "---\n",
        "\n",
        "# **Summary**\n",
        "\n",
        "* **Naïve Bayes** is a classifier based on Bayes’ Theorem.\n",
        "* It is called **\"naïve\"** because it assumes **independence among features**, which is rarely true.\n",
        "* Despite this, it works very well in practice.\n",
        "\n",
        "---\n",
        "\n",
        "If you’d like, I can explain the **types of Naïve Bayes classifiers** (Gaussian, Multinomial, Bernoulli) or provide a **Python implementation**.\n"
      ],
      "metadata": {
        "id": "0O4QMcj8mKyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "Here are the **clear and concise differences** between **Gaussian**, **Multinomial**, and **Bernoulli** Naïve Bayes classifiers:\n",
        "\n",
        "---\n",
        "\n",
        "#  **1. Gaussian Naïve Bayes**\n",
        "\n",
        "### **Used for:**\n",
        "\n",
        "Continuous (real-valued) features.\n",
        "\n",
        "### **Assumption:**\n",
        "\n",
        "Features follow a **Gaussian (normal) distribution**.\n",
        "\n",
        "[\n",
        "P(x_i | C) = \\text{Gaussian distribution}\n",
        "]\n",
        "\n",
        "### **Examples of suitable data:**\n",
        "\n",
        "* Iris flower measurements\n",
        "* Sensor readings\n",
        "* Any numeric features\n",
        "\n",
        "### **Common use cases:**\n",
        "\n",
        "General machine learning tasks with continuous data.\n",
        "\n",
        "---\n",
        "\n",
        "#  **2. Multinomial Naïve Bayes**\n",
        "\n",
        "### **Used for:**\n",
        "\n",
        "Discrete (count-based) features.\n",
        "\n",
        "### **Assumption:**\n",
        "\n",
        "Features represent **counts** or **frequency** of events.\n",
        "\n",
        "[\n",
        "x_i \\in {0,1,2,\\ldots}\n",
        "]\n",
        "\n",
        "### **Examples of suitable data:**\n",
        "\n",
        "* Word counts in text (Bag-of-Words)\n",
        "* Term frequency vectors\n",
        "* Document classification\n",
        "\n",
        "### **Common use cases:**\n",
        "\n",
        "* Spam detection\n",
        "* Text classification\n",
        "* NLP problems\n",
        "\n",
        "---\n",
        "\n",
        "#  **3. Bernoulli Naïve Bayes**\n",
        "\n",
        "### **Used for:**\n",
        "\n",
        "Binary (0/1 or True/False) features.\n",
        "\n",
        "### **Assumption:**\n",
        "\n",
        "Each feature is **boolean**, indicating presence/absence of something.\n",
        "\n",
        "[\n",
        "x_i \\in {0, 1}\n",
        "]\n",
        "\n",
        "### **Examples of suitable data:**\n",
        "\n",
        "* Binary word occurrence features\n",
        "* Whether a word exists in a document\n",
        "* Yes/No attributes\n",
        "\n",
        "### **Common use cases:**\n",
        "\n",
        "* Text classification where features are binary\n",
        "* Sentiment analysis with presence/absence of keywords\n",
        "\n",
        "---\n",
        "\n",
        "#  **Side-by-Side Comparison**\n",
        "\n",
        "| Feature Type   | Gaussian NB              | Multinomial NB           | Bernoulli NB                      |\n",
        "| -------------- | ------------------------ | ------------------------ | --------------------------------- |\n",
        "| Feature Nature | Continuous (real values) | Counts, frequencies      | Binary (0/1)                      |\n",
        "| Assumes        | Normal distribution      | Multinomial distribution | Bernoulli distribution            |\n",
        "| Examples       | Height, weight           | Word counts              | Word presence                     |\n",
        "| Common Use     | General ML tasks         | Text classification      | Text classification (binary form) |\n",
        "| Input Type     | Float                    | Non-negative integers    | Binary values                     |\n",
        "\n",
        "---\n",
        "\n",
        "#  **Summary**\n",
        "\n",
        "* **Gaussian NB** → continuous features\n",
        "* **Multinomial NB** → count-based features (common in NLP)\n",
        "* **Bernoulli NB** → binary features (presence/absence)\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OlSIjJcPmXm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancerdataset and evaluate accuracy.\n",
        "#Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset fromsklearn.datasets.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN0EJojgmw31",
        "outputId": "513b5b4e-82b4-4799-8e43-b820081d9356"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: 0.9737\n"
          ]
        }
      ]
    }
  ]
}